
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/1model.ipynb

import torch
from torch import nn

from exp.nb_01 import GeneralRelu

class Lambda(nn.Module):
    def __init__(self,func):
        super().__init__()
        self.f = func
        self.init_shape = None

    def forward(self,*inp):
#         shape = mu.shape
        if self.init_shape is None: self.init_shape = inp[0].shape
        return self.f(*inp)

def flatten(x): return x.view(x.shape[0], -1)

def sampler(mu,log_var):
    sigma = torch.randn(*mu.shape)
    return mu + sigma.to(mu.device)*torch.exp(log_var/2)

def change_view(*size):
    def _inner(x): return x.view(*((-1,) + size))
    return _inner

def conv_layer(ni,nf,ks = 3, s = 2, bn =True,dropout = True,**kwargs):
    layers = []
    layers += [nn.Conv2d(ni,nf,kernel_size = ks, stride = s, padding = ks//2, bias = not bn), GeneralRelu(**kwargs)]
    if bn:
        layers.append(nn.BatchNorm2d(nf))
    if dropout:
        layers.append(nn.Dropout2d(0.25))
    return nn.Sequential(*layers)


def conv_model(channels,layer = conv_layer,bn = True, dropout = True,**kwargs):
    channels = [3] + channels
    layers = [layer(channels[i], channels[i+1], ks = 3, s = 2,bn = bn, dropout = dropout,**kwargs)
              for i in range(len(channels)-1)]
    return nn.Sequential(*layers)

class Encoder(nn.Module):
    def __init__(self,channels,layer = conv_layer,bn = True,dropout = True,z_dim = 200, **kwargs):
        super().__init__()
        self.channels = channels
        self.z = z_dim
        self.layer = layer
        self.bn = bn
        self.drop = dropout
        self.dict = kwargs
        self.shape_before_flattening = None
        self._build()

    def _build(self):
        self.model = conv_model(self.channels,self.layer,bn = self.bn, dropout=self.drop,**self.dict)
        self.model.add_module('flatten', Lambda(flatten))
#         self.model.flatten.add_module('mu', nn.Linear())

    def forward(self,x):
        x = self.model(x)
        if not list(self.model.flatten.children()):
            self.shape_before_flattening = self.model.flatten.init_shape
            self.model.flatten.add_module('mu', nn.Linear(x.shape[1], self.z))
            self.model.flatten.add_module('lvar', nn.Linear(x.shape[1], self.z))
#         x = Lambda(sampler)(self.model.flatten.mu(x),self.model.flatten.lvar(x))
        if 'mean' not in list(i.name for i in self.buffers()):
            self.register_buffer('mean', torch.zeros(x.shape[0],self.z))
            self.register_buffer('log_var', torch.zeros(x.shape[0],self.z))
        self.mean = self.model.flatten.mu(x)
        self.log_var = self.model.flatten.lvar(x)
        x = Lambda(sampler)(self.mean,self.log_var)
        return x

#         self.channels  = [1] + self.channels

def conv_transpose_layer(ni,nf,ks = 3, s = 2, bn =True,output_padding = 1,dropout = True,**kwargs):
    layers = []
    layers += [nn.ConvTranspose2d(ni,nf,kernel_size = ks, stride = s, padding = ks//2, output_padding = output_padding,
                                  bias = not bn), GeneralRelu(**kwargs)]
    if bn:
        layers.append(nn.BatchNorm2d(nf))
    if dropout:
        layers.append(nn.Dropout2d(0.25))
    return nn.Sequential(*layers)

def conv_transpose_model(channels,layer = conv_transpose_layer,bn = True, outp = 1,dropout = True,**kwargs):
    channels.append(3)
    layers = [layer(channels[i], channels[i+1], ks = 3, s = 2,bn = bn if i < len(channels)-2 else False,
                    output_padding = outp,dropout = dropout if i < len(channels)-2 else False,**kwargs)
              for i in range(len(channels)-1)]
    return nn.Sequential(*layers)

class Decoder(nn.Module):
    def __init__(self,channels,shape_before_flattening,layer = conv_transpose_layer,bn = True, z_dim = 200,dropout = True,**kwargs):
        super().__init__()
        self.channels = channels
        self.z = z_dim
        self.layer = layer
        self.bn = bn
        self.drop = dropout
        self.dict = kwargs
        self.init_shape =  shape_before_flattening
        self.flatten_func = change_view(*tuple(self.init_shape[1:]))
        self._build()

    def _build(self):
        self.model = nn.Sequential(nn.Linear(self.z, int(self.init_shape.numel()/self.init_shape[0])))
        self.model.add_module('deflatten', Lambda(self.flatten_func))
        self.model.add_module('transpose_model',conv_transpose_model(self.channels,self.layer,bn = self.bn, dropout = self.drop,**self.dict))

#         self.model.flatten.add_module('mu', nn.Linear())

    def forward(self,x):
        x = self.model(x)
#         if not list(self.model.flatten.children()):
#             self.model.flatten.add_module('mu', nn.Linear(x.shape[1], self.z))
#             self.model.flatten.add_module('lvar', nn.Linear(x.shape[1], self.z))
#         x = Lambda(sampler)(self.model.flatten.mu(x),self.model.flatten.lvar(x))
        return x

#         self.channels  = [1] + self.channels

class Variational_Autoencoder(nn.Module):
    def __init__(self,enc_channels,dec_channels,bn=True,z_dim = 200,enc_layer = conv_layer,dec_layer = conv_transpose_layer,
                 dropout = True,**kwargs):
        super().__init__()
        self.dc_channels, self.dc_layer,self.bn,self.z, self.dict = dec_channels,dec_layer,bn,z_dim,kwargs
        self.dc_drop = dropout
        self.enc = Encoder(enc_channels,layer = enc_layer,bn = bn,z_dim = z_dim,dropout= dropout,**kwargs)
        self.dec = None

    def forward(self,x):
        x = self.enc(x)
        if self.dec is None:
            self.dec = Decoder(self.dc_channels,self.enc.shape_before_flattening,self.dc_layer,bn = self.bn,z_dim = self.z,dropout= self.dc_drop,**self.dict)
        x = self.dec(x)
        return x


